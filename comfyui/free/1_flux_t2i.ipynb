{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content\n",
    "%cd /content\n",
    "\n",
    "# Clone the ComfyUI repository from GitHub\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git ./ComfyUI\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git ./ComfyUI/custom_nodes/quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "# Install library\n",
    "!pip install -q einops==0.8.0 torchsde==0.2.6 diffusers==0.31.0 accelerate==1.1.0 gguf==0.10.0 gradio==5.9.1\n",
    "!pip install -q huggingface_hub==0.27.0 hf_transfer==0.1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained models using huggingface-cli\n",
    "\n",
    "# Enable faster downloads with HF_HUB_ENABLE_HF_TRANSFER\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download martintomov/Hyper-FLUX.1-dev-gguf hyper-flux-16step-Q4_K_M.gguf --local-dir ./models/unet --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download camenduru/FLUX.1-dev ae.sft --local-dir ./models/vae --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download zer0int/CLIP-GmP-ViT-L-14 ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors --local-dir ./models/clip --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp8_e4m3fn.safetensors --local-dir ./models/clip --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "# ComfyUI imports\n",
    "import nodes\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy import model_management\n",
    "from comfy_extras import nodes_custom_sampler, nodes_flux, nodes_model_advanced\n",
    "\n",
    "# Custom Node imports\n",
    "from custom_nodes.quantized.nodes import NODE_CLASS_MAPPINGS as Q_NODE_CLASS_MAPPINGS\n",
    "\n",
    "# Global model references (loaded once in load_models)\n",
    "clip = None\n",
    "vae = None\n",
    "unet = None\n",
    "\n",
    "# Global node references (for Sampler, Guider, etc.)\n",
    "DualCLIPLoader = None\n",
    "VAELoader = None\n",
    "CLIPTextEncode = None\n",
    "FluxGuidance = None\n",
    "ModelSamplingFlux = None\n",
    "BasicGuider = None\n",
    "KSamplerSelect = None\n",
    "BasicScheduler = None\n",
    "SamplerCustomAdvanced = None\n",
    "RandomNoise = None\n",
    "EmptyLatentImage = None\n",
    "VAEDecode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Loads all necessary models and node references for ComfyUI usage.\n",
    "    \"\"\"\n",
    "    global clip, vae, unet\n",
    "    global DualCLIPLoader, VAELoader, CLIPTextEncode\n",
    "    global FluxGuidance, ModelSamplingFlux, BasicGuider\n",
    "    global KSamplerSelect, BasicScheduler, SamplerCustomAdvanced\n",
    "    global RandomNoise, EmptyLatentImage, VAEDecode\n",
    "\n",
    "    # Instantiate ComfyUI nodes\n",
    "    DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "    VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "    CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "    FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "    ModelSamplingFlux = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "    BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "    KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "    BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "    SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "\n",
    "    RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "    EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "    VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "    # Load CLIP and VAE\n",
    "    with torch.inference_mode():\n",
    "        clip = DualCLIPLoader.load_clip(\n",
    "            \"t5xxl_fp8_e4m3fn.safetensors\",\n",
    "            \"ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors\",\n",
    "            \"flux\"\n",
    "        )[0]\n",
    "        vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "\n",
    "    # Load UNet\n",
    "    UNETLoader = Q_NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
    "    with torch.inference_mode():\n",
    "        unet = UNETLoader.load_unet(\"hyper-flux-16step-Q4_K_M.gguf\")[0]\n",
    "\n",
    "\n",
    "def closestNumber(n, m):\n",
    "    \"\"\"\n",
    "    Finds the closest multiple of m to the number n.\n",
    "    \"\"\"\n",
    "    q = int(n / m)\n",
    "    n1 = m * q\n",
    "    if (n * m) > 0:\n",
    "        n2 = m * (q + 1)\n",
    "    else:\n",
    "        n2 = m * (q - 1)\n",
    "\n",
    "    if abs(n - n1) < abs(n - n2):\n",
    "        return n1\n",
    "    return n2\n",
    "\n",
    "\n",
    "def generate_image(prompt, width, height, seed):\n",
    "    \"\"\"\n",
    "    Generates the image using the loaded models based on the given prompt, width, height, and seed.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        positive_prompt = prompt\n",
    "        steps = 16\n",
    "        sampler_name = \"euler\"\n",
    "        scheduler = \"simple\"\n",
    "\n",
    "        cond = CLIPTextEncode.encode(text=positive_prompt, clip=clip)[0]\n",
    "        cond_f = FluxGuidance.append(conditioning=cond, guidance=3.5)[0]\n",
    "\n",
    "        noise = RandomNoise.get_noise(seed)[0]\n",
    "        new_unet = ModelSamplingFlux.patch(\n",
    "            model=unet,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            max_shift=1.15,\n",
    "            base_shift=0.5\n",
    "        )[0]\n",
    "\n",
    "        sampler = KSamplerSelect.get_sampler(sampler_name=sampler_name)[0]\n",
    "        guider = BasicGuider.get_guider(model=new_unet, conditioning=cond_f)[0]\n",
    "        sigmas = BasicScheduler.get_sigmas(model=new_unet, scheduler=scheduler, steps=steps, denoise=1.0)[0]\n",
    "        latent_image = EmptyLatentImage.generate(\n",
    "            closestNumber(width, 16),\n",
    "            closestNumber(height, 16)\n",
    "        )[0]\n",
    "\n",
    "        sample, _ = SamplerCustomAdvanced.sample(\n",
    "            noise=noise,\n",
    "            guider=guider,\n",
    "            sampler=sampler,\n",
    "            sigmas=sigmas,\n",
    "            latent_image=latent_image\n",
    "        )\n",
    "        model_management.soft_empty_cache()\n",
    "\n",
    "        decoded = VAEDecode.decode(vae=vae, samples=sample)[0].detach()\n",
    "        image = Image.fromarray(np.array(decoded * 255, dtype=np.uint8)[0])\n",
    "        return image\n",
    "\n",
    "\n",
    "def on_generate_click(prompt, width, height, seed):\n",
    "    \"\"\"\n",
    "    Callback function for the '이미지 생성하기' button in Gradio.\n",
    "    Attempts to generate the image and returns the result and status.\n",
    "    \"\"\"\n",
    "    if not prompt.strip():\n",
    "        return None, \"❌ 텍스트를 입력해주세요.\", None\n",
    "    try:\n",
    "        image = generate_image(prompt, width, height, seed)\n",
    "        return image, \"✅ 성공적으로 이미지를 생성했습니다.\", seed\n",
    "    except Exception as e:\n",
    "        return None, f\"❌ 에러 발생: {str(e)}\", None\n",
    "\n",
    "\n",
    "def roll_seed():\n",
    "    \"\"\"\n",
    "    Generates a random seed.\n",
    "    \"\"\"\n",
    "    return random.randint(0, 2**32 - 1)\n",
    "\n",
    "\n",
    "def create_interface():\n",
    "    css = \"\"\"\n",
    "    footer {\n",
    "        visibility: hidden;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=on_generate_click,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"📝 텍스트\",\n",
    "                placeholder=\"원하는 장면을 영어로 입력해주세요...\",\n",
    "                lines=3\n",
    "            ),\n",
    "            gr.Slider(\n",
    "                label=\"📏 가로 (pixels)\",\n",
    "                minimum=64,\n",
    "                maximum=1280,\n",
    "                step=64,\n",
    "                value=1024\n",
    "            ),\n",
    "            gr.Slider(\n",
    "                label=\"📏 세로 (pixels)\",\n",
    "                minimum=64,\n",
    "                maximum=1280,\n",
    "                step=64,\n",
    "                value=1024\n",
    "            ),\n",
    "            gr.Slider(0, 2**32 -1, 2024, label=\"🎲 시드\", step=1)\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Image(label=\"🖼️ 생성한 이미지\"),\n",
    "            gr.Textbox(label=\"🔄 현재 상태\", interactive=False),\n",
    "            gr.Number(label=\"📌 사용된 시드\", value=42, precision=0, interactive=False)\n",
    "        ],\n",
    "        title=\"🖼️ 플럭스 활용하기\",\n",
    "        theme='JohnSmith9982/small_and_pretty',\n",
    "        css=css,\n",
    "        allow_flagging=\"never\",\n",
    "    )\n",
    "\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load all models first\n",
    "    load_models()\n",
    "\n",
    "    # Create and launch Gradio interface\n",
    "    interface = create_interface()\n",
    "    interface.launch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
