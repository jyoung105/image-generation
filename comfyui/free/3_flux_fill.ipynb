{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone code\n",
    "%cd /content\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git ./ComfyUI\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git ./ComfyUI/custom_nodes/quantized\n",
    "\n",
    "# Download library\n",
    "%cd /content/ComfyUI\n",
    "!pip install -q einops==0.8.0 torchsde==0.2.6 diffusers==0.31.0 accelerate==1.1.0 gguf==0.10.0 gradio==5.8.0\n",
    "!pip install -q huggingface_hub==0.27.0 hf_transfer==0.1.8\n",
    "\n",
    "# Download model\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download YarvixPA/FLUX.1-Fill-dev-gguf flux1-fill-dev-Q4_K_S.gguf --local-dir ./models/unet --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download camenduru/FLUX.1-dev ae.sft --local-dir ./models/vae --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download zer0int/CLIP-GmP-ViT-L-14 ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors --local-dir ./models/clip --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp8_e4m3fn.safetensors --local-dir ./models/clip --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nodes\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "import torch\n",
    "import nodes\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy import model_management\n",
    "from comfy_extras import nodes_custom_sampler, nodes_differential_diffusion, nodes_flux, nodes_model_advanced\n",
    "\n",
    "CLIPVisionLoader = nodes.NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "LoadImage = nodes.NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "InpaintModelConditioning = nodes.NODE_CLASS_MAPPINGS[\"InpaintModelConditioning\"]()\n",
    "DifferentialDiffusion = nodes_differential_diffusion.NODE_CLASS_MAPPINGS[\"DifferentialDiffusion\"]()\n",
    "\n",
    "DualCLIPLoader = nodes.NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "VAELoader = nodes.NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "CLIPTextEncode = nodes.NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "KSampler = nodes.NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
    "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "\n",
    "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "EmptyLatentImage = nodes.NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "VAEDecode = nodes.NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "# Import model\n",
    "with torch.inference_mode():\n",
    "  clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors\", \"flux\")[0]\n",
    "  vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "\n",
    "from custom_nodes.quantized.nodes import NODE_CLASS_MAPPINGS\n",
    "\n",
    "UNETLoader = NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  unet = UNETLoader.load_unet(\"flux1-fill-dev-Q4_K_S.gguf\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import gradio as gr\n",
    "\n",
    "def closestNumber(n, m):\n",
    "    q = int(n / m)\n",
    "    n1 = m * q\n",
    "    if (n * m) > 0:\n",
    "        n2 = m * (q + 1)\n",
    "    else:\n",
    "        n2 = m * (q - 1)\n",
    "    if abs(n - n1) < abs(n - n2):\n",
    "        return n1\n",
    "    return n2\n",
    "\n",
    "def resize_max(image):\n",
    "    max_length = 1024\n",
    "    width, height = image.size\n",
    "    scaling_factor = min(max_length / width, max_length / height)\n",
    "\n",
    "    if scaling_factor < 1:\n",
    "        new_width = int(width * scaling_factor)\n",
    "        new_height = int(height * scaling_factor)\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Function to generate image\n",
    "def generate_image(image, prompt, seed):\n",
    "  with torch.inference_mode():\n",
    "    positive_prompt = prompt\n",
    "    seed = seed\n",
    "    steps = 20\n",
    "    sampler_name = \"euler\"\n",
    "    scheduler = \"simple\"\n",
    "\n",
    "    image_path = image['background']\n",
    "    mask_path = image['layers'][0]\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    image.save(\"/content/ComfyUI/input/original.png\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    mask = ImageOps.invert(mask)\n",
    "    mask.save(\"/content/ComfyUI/input/mask.png\")\n",
    "\n",
    "    width, height = image.size\n",
    "    image_f = resize_max(image)\n",
    "    mask_f = resize_max(mask)\n",
    "    image_f.putalpha(mask_f)\n",
    "    image_f.save(\"/content/ComfyUI/input/image.png\")\n",
    "\n",
    "    image_ff, mask_ff = LoadImage.load_image(\"image.png\") # \"image.png\"\n",
    "\n",
    "    cond = CLIPTextEncode.encode(text=prompt, clip=clip)[0]\n",
    "    cond_f = FluxGuidance.append(conditioning=cond, guidance=30)[0]\n",
    "    con_f_neg = CLIPTextEncode.encode(text=\"\", clip=clip)[0]\n",
    "    cond_inpaint, cond_neg_inpaint, latent = InpaintModelConditioning.encode(positive=cond_f, negative=con_f_neg, vae=vae, pixels=image_ff, mask=mask_ff, noise_mask=False)\n",
    "\n",
    "    noise = RandomNoise.get_noise(seed)[0]\n",
    "    new_unet = DifferentialDiffusion.apply(model=unet)[0]\n",
    "    sample = KSampler.sample(model=new_unet, positive=cond_inpaint, negative=cond_neg_inpaint, latent_image=latent, seed=seed, steps=steps, cfg=1.0, sampler_name=sampler_name, scheduler=scheduler, denoise=1.0)[0]\n",
    "    model_management.soft_empty_cache()\n",
    "\n",
    "    decoded = VAEDecode.decode(vae=vae, samples=sample)[0].detach()\n",
    "    image = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])\n",
    "\n",
    "    return image\n",
    "\n",
    "# Function to handle button click\n",
    "def on_generate_click(image, prompt, seed):\n",
    "    if not prompt.strip():\n",
    "        return None, \"❌ 텍스트를 입력해주세요.\", None\n",
    "    if not image:\n",
    "        return None, \"❌ 이미지를 입력해주세요.\", None\n",
    "    try:\n",
    "        image = generate_image(image, prompt, seed)\n",
    "        return image, \"✅ 성공적으로 이미지를 생성했습니다.\", seed\n",
    "    except Exception as e:\n",
    "        return None, f\"❌ 에러 발생: {str(e)}\", None\n",
    "\n",
    "# Function to generate a random seed\n",
    "def roll_seed():\n",
    "    return random.randint(0, 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "def create_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# 🖼️ 플럭스 활용하기\")\n",
    "        with gr.Row():\n",
    "            # Left column: Inputs\n",
    "            with gr.Column():\n",
    "                image = gr.ImageEditor(\n",
    "                    label=\"🖼️ 이미지\",\n",
    "                    type='filepath',\n",
    "                    sources=\"upload\",\n",
    "                    brush=gr.Brush(colors=[\"#FFFFFF\"], default_color=\"#FFFFFF\", color_mode=\"fixed\"),\n",
    "                    height=1000,\n",
    "                )\n",
    "                prompt = gr.Textbox(\n",
    "                    label=\"📝 텍스트\",\n",
    "                    placeholder=\"원하는 장면을 영어로 입력해주세요...\",\n",
    "                    lines=3\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    seed = gr.Number(\n",
    "                        label=\"🎲 시드\",\n",
    "                        value=2024,\n",
    "                        precision=0,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    roll_seed_btn = gr.Button(\"🎲 랜덤 시드 만들기\")\n",
    "                generate_btn = gr.Button(\"🎨 이미지 생성하기\")\n",
    "                progress = gr.Textbox(label=\"🔄 현재 상태\", interactive=False)\n",
    "\n",
    "            # Right column: Output\n",
    "            with gr.Column(scale=1):\n",
    "                output = gr.Image(label=\"🖼️ 생성한 이미지\")\n",
    "                used_seed = gr.Number(label=\"📌 사용된 시드\", value=42, precision=0, interactive=False)\n",
    "\n",
    "        # Define the button click events\n",
    "        generate_btn.click(\n",
    "            fn=on_generate_click,\n",
    "            inputs=[image, prompt, seed],\n",
    "            outputs=[output, progress, used_seed]\n",
    "        )\n",
    "\n",
    "        roll_seed_btn.click(\n",
    "            fn=roll_seed,\n",
    "            inputs=None,\n",
    "            outputs=seed\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_interface()\n",
    "    interface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
