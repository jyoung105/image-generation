{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content\n",
    "%cd /content\n",
    "\n",
    "# Clone the ComfyUI repository from GitHub\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git ./ComfyUI\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git ./ComfyUI/custom_nodes/quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "# Install library\n",
    "!pip install -q einops==0.8.0 torchsde==0.2.6 diffusers==0.31.0 accelerate==1.1.0 gguf==0.10.0 gradio==5.8.0\n",
    "!pip install -q huggingface_hub==0.27.0 hf_transfer==0.1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained models using huggingface-cli\n",
    "\n",
    "# Enable faster downloads with HF_HUB_ENABLE_HF_TRANSFER\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download YarvixPA/FLUX.1-Fill-dev-gguf flux1-fill-dev-Q4_K_S.gguf --local-dir ./models/unet --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download camenduru/FLUX.1-dev ae.sft --local-dir ./models/vae --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download zer0int/CLIP-GmP-ViT-L-14 ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors --local-dir ./models/clip --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp8_e4m3fn.safetensors --local-dir ./models/clip --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import gradio as gr\n",
    "\n",
    "# ComfyUI imports\n",
    "import nodes\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy import model_management\n",
    "from comfy_extras import (\n",
    "    nodes_custom_sampler,\n",
    "    nodes_differential_diffusion,\n",
    "    nodes_flux,\n",
    "    nodes_model_advanced\n",
    ")\n",
    "\n",
    "# Custom Node imports\n",
    "from custom_nodes.quantized.nodes import NODE_CLASS_MAPPINGS as Q_NODE_CLASS_MAPPINGS\n",
    "\n",
    "# Global references for nodes/models\n",
    "CLIPVisionLoader = None\n",
    "LoadImage = None\n",
    "InpaintModelConditioning = None\n",
    "DifferentialDiffusion = None\n",
    "DualCLIPLoader = None\n",
    "VAELoader = None\n",
    "CLIPTextEncode = None\n",
    "FluxGuidance = None\n",
    "BasicGuider = None\n",
    "KSampler = None\n",
    "BasicScheduler = None\n",
    "SamplerCustomAdvanced = None\n",
    "RandomNoise = None\n",
    "EmptyLatentImage = None\n",
    "VAEDecode = None\n",
    "\n",
    "clip = None\n",
    "vae = None\n",
    "unet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Initializes and loads all required ComfyUI nodes and models.\n",
    "    Sets global references to nodes and loaded models.\n",
    "    \"\"\"\n",
    "    global CLIPVisionLoader, LoadImage, InpaintModelConditioning\n",
    "    global DifferentialDiffusion, DualCLIPLoader, VAELoader\n",
    "    global CLIPTextEncode, FluxGuidance, BasicGuider, KSampler\n",
    "    global BasicScheduler, SamplerCustomAdvanced, RandomNoise\n",
    "    global EmptyLatentImage, VAEDecode\n",
    "    global clip, vae, unet\n",
    "\n",
    "    # Instantiate node classes\n",
    "    CLIPVisionLoader = NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "    LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "    InpaintModelConditioning = NODE_CLASS_MAPPINGS[\"InpaintModelConditioning\"]()\n",
    "    DifferentialDiffusion = nodes_differential_diffusion.NODE_CLASS_MAPPINGS[\"DifferentialDiffusion\"]()\n",
    "\n",
    "    DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "    VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "    CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "    FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "    BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "    KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
    "    BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "    SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "\n",
    "    RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "    EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "    VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "    # Load models\n",
    "    with torch.inference_mode():\n",
    "        # Load CLIP and VAE\n",
    "        clip_model = DualCLIPLoader.load_clip(\n",
    "            \"t5xxl_fp8_e4m3fn.safetensors\",\n",
    "            \"ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors\",\n",
    "            \"flux\"\n",
    "        )[0]\n",
    "        vae_model = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "\n",
    "        # Load UNet\n",
    "        UNETLoader = Q_NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
    "        unet_model = UNETLoader.load_unet(\"flux1-fill-dev-Q4_K_S.gguf\")[0]\n",
    "\n",
    "    # Assign globally\n",
    "    global clip, vae, unet\n",
    "    clip = clip_model\n",
    "    vae = vae_model\n",
    "    unet = unet_model\n",
    "\n",
    "\n",
    "def closestNumber(n, m):\n",
    "    \"\"\"\n",
    "    Finds the closest multiple of m to the number n.\n",
    "    \"\"\"\n",
    "    q = int(n / m)\n",
    "    n1 = m * q\n",
    "    if (n * m) > 0:\n",
    "        n2 = m * (q + 1)\n",
    "    else:\n",
    "        n2 = m * (q - 1)\n",
    "    if abs(n - n1) < abs(n - n2):\n",
    "        return n1\n",
    "    return n2\n",
    "\n",
    "\n",
    "def resize_max(image):\n",
    "    \"\"\"\n",
    "    Resizes the image so that its longest side does not exceed 1024 pixels.\n",
    "    \"\"\"\n",
    "    max_length = 1024\n",
    "    width, height = image.size\n",
    "    scaling_factor = min(max_length / width, max_length / height)\n",
    "\n",
    "    if scaling_factor < 1:\n",
    "        new_width = int(width * scaling_factor)\n",
    "        new_height = int(height * scaling_factor)\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_image(image, prompt, seed):\n",
    "    \"\"\"\n",
    "    Generates an inpainted image using the loaded ComfyUI nodes and models.\n",
    "    Takes an RGBA input image with mask information, a text prompt, and a seed.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        steps = 20\n",
    "        sampler_name = \"euler\"\n",
    "        scheduler = \"simple\"\n",
    "\n",
    "        image_path = image[\"background\"]\n",
    "        mask_path = image[\"layers\"][0]\n",
    "\n",
    "        # Convert input image and mask\n",
    "        original_image = Image.open(image_path).convert(\"RGBA\")\n",
    "        original_image.save(\"/content/ComfyUI/input/original.png\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        mask = ImageOps.invert(mask)  # Invert mask for inpainting\n",
    "        mask.save(\"/content/ComfyUI/input/mask.png\")\n",
    "\n",
    "        # Resize if needed\n",
    "        width, height = original_image.size\n",
    "        image_f = resize_max(original_image)\n",
    "        mask_f = resize_max(mask)\n",
    "\n",
    "        # Combine image with mask in alpha channel\n",
    "        image_f.putalpha(mask_f)\n",
    "        image_f.save(\"/content/ComfyUI/input/image.png\")\n",
    "\n",
    "        # Load the processed image into the ComfyUI pipeline\n",
    "        image_ff, mask_ff = LoadImage.load_image(\"image.png\")\n",
    "\n",
    "        # Encode textual prompt and apply flux guidance\n",
    "        cond = CLIPTextEncode.encode(text=prompt, clip=clip)[0]\n",
    "        cond_f = FluxGuidance.append(conditioning=cond, guidance=30)[0]\n",
    "\n",
    "        con_f_neg = CLIPTextEncode.encode(text=\"\", clip=clip)[0]\n",
    "\n",
    "        # Prepare inpaint conditioning\n",
    "        cond_inpaint, cond_neg_inpaint, latent = InpaintModelConditioning.encode(\n",
    "            positive=cond_f,\n",
    "            negative=con_f_neg,\n",
    "            vae=vae,\n",
    "            pixels=image_ff,\n",
    "            mask=mask_ff,\n",
    "            noise_mask=False\n",
    "        )\n",
    "\n",
    "        noise = RandomNoise.get_noise(seed)[0]\n",
    "\n",
    "        # Apply differential diffusion to the UNet model\n",
    "        new_unet = DifferentialDiffusion.apply(model=unet)[0]\n",
    "\n",
    "        # Perform sampling\n",
    "        sample = KSampler.sample(\n",
    "            model=new_unet,\n",
    "            positive=cond_inpaint,\n",
    "            negative=cond_neg_inpaint,\n",
    "            latent_image=latent,\n",
    "            seed=seed,\n",
    "            steps=steps,\n",
    "            cfg=1.0,\n",
    "            sampler_name=sampler_name,\n",
    "            scheduler=scheduler,\n",
    "            denoise=1.0\n",
    "        )[0]\n",
    "        model_management.soft_empty_cache()\n",
    "\n",
    "        # Decode from latent\n",
    "        decoded = VAEDecode.decode(vae=vae, samples=sample)[0].detach()\n",
    "        output_image = Image.fromarray(\n",
    "            np.array(decoded * 255, dtype=np.uint8)[0]\n",
    "        )\n",
    "\n",
    "        return output_image\n",
    "\n",
    "\n",
    "def on_generate_click(image, prompt, seed):\n",
    "    \"\"\"\n",
    "    Callback function for the '이미지 생성하기' button in Gradio.\n",
    "    Attempts to generate the image and returns the result and status.\n",
    "    \"\"\"\n",
    "    if not prompt.strip():\n",
    "        return None, \"❌ 텍스트를 입력해주세요.\", None\n",
    "    if not image:\n",
    "        return None, \"❌ 이미지를 입력해주세요.\", None\n",
    "    try:\n",
    "        generated = generate_image(image, prompt, seed)\n",
    "        return generated, \"✅ 성공적으로 이미지를 생성했습니다.\", seed\n",
    "    except Exception as e:\n",
    "        return None, f\"❌ 에러 발생: {str(e)}\", None\n",
    "\n",
    "\n",
    "def roll_seed():\n",
    "    \"\"\"\n",
    "    Generates a random seed.\n",
    "    \"\"\"\n",
    "    return random.randint(0, 2**32 - 1)\n",
    "\n",
    "\n",
    "def create_interface():\n",
    "    css = \"\"\"\n",
    "    footer {\n",
    "        visibility: hidden;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=on_generate_click,\n",
    "        inputs=[\n",
    "            gr.ImageEditor(\n",
    "                label=\"🖼️ 이미지\",\n",
    "                type='filepath',\n",
    "                sources=\"upload\",\n",
    "                brush=gr.Brush(\n",
    "                    colors=[\"#FFFFFF\"],\n",
    "                    default_color=\"#FFFFFF\",\n",
    "                    color_mode=\"fixed\"\n",
    "                ),\n",
    "                height=\"100%\"\n",
    "            ),\n",
    "            gr.Textbox(\n",
    "                label=\"📝 텍스트\",\n",
    "                placeholder=\"원하는 장면을 영어로 입력해주세요...\",\n",
    "                lines=3\n",
    "            ),\n",
    "            gr.Slider(0, 2**32 -1, 2024, label=\"🎲 시드\", step=1),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Image(label=\"🖼️ 생성한 이미지\"),\n",
    "            gr.Textbox(label=\"🔄 현재 상태\", interactive=False),\n",
    "            gr.Number(label=\"📌 사용된 시드\", precision=0, interactive=False)\n",
    "        ],\n",
    "        title=\"🖼️ 플럭스 활용하기\",\n",
    "        theme='JohnSmith9982/small_and_pretty',\n",
    "        css=css,\n",
    "        allow_flagging=\"never\"\n",
    "    )\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load all models first\n",
    "    load_models()\n",
    "\n",
    "    # Create and launch the Gradio interface\n",
    "    interface = create_interface()\n",
    "    interface.launch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
