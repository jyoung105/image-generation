{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content\n",
    "%cd /content\n",
    "\n",
    "# Clone the ComfyUI repository from GitHub\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git ./ComfyUI\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git ./ComfyUI/custom_nodes/quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "# Install library\n",
    "!pip install -q einops==0.8.0 torchsde==0.2.6 diffusers==0.31.0 accelerate==1.1.0 gguf==0.10.0 gradio==5.9.1\n",
    "!pip install -q huggingface_hub==0.27.0 hf_transfer==0.1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained models using huggingface-cli\n",
    "\n",
    "# Enable faster downloads with HF_HUB_ENABLE_HF_TRANSFER\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download martintomov/Hyper-FLUX.1-dev-gguf hyper-flux-16step-Q4_K_M.gguf --local-dir ./models/unet --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Comfy-Org/sigclip_vision_384 sigclip_vision_patch14_384.safetensors --local-dir ./models/clip_vision --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Thelocallab/Flux-Dev-Redux flux1-redux-dev.safetensors --local-dir ./models/style_models --local-dir-use-symlinks False # originally, it's from black-forest-labs/FLUX.1-Redux-dev. Check the LICENSE.\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download camenduru/FLUX.1-dev ae.sft --local-dir ./models/vae --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download zer0int/CLIP-GmP-ViT-L-14 ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors --local-dir ./models/clip --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp8_e4m3fn.safetensors --local-dir ./models/clip --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current directory to /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "# ComfyUI imports\n",
    "import nodes\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy import model_management\n",
    "from comfy_extras import nodes_custom_sampler, nodes_flux, nodes_model_advanced\n",
    "\n",
    "# Custom Node imports\n",
    "from custom_nodes.quantized.nodes import NODE_CLASS_MAPPINGS as Q_NODE_CLASS_MAPPINGS\n",
    "\n",
    "# Global model/node references\n",
    "clip = None\n",
    "vae = None\n",
    "unet = None\n",
    "clip_vision = None\n",
    "style_model = None\n",
    "\n",
    "CLIPVisionLoader = None\n",
    "LoadImage = None\n",
    "StyleModelLoader = None\n",
    "CLIPVisionEncode = None\n",
    "StyleModelApply = None\n",
    "\n",
    "DualCLIPLoader = None\n",
    "VAELoader = None\n",
    "CLIPTextEncode = None\n",
    "FluxGuidance = None\n",
    "ModelSamplingFlux = None\n",
    "BasicGuider = None\n",
    "KSamplerSelect = None\n",
    "BasicScheduler = None\n",
    "SamplerCustomAdvanced = None\n",
    "\n",
    "RandomNoise = None\n",
    "EmptyLatentImage = None\n",
    "VAEDecode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Loads all the necessary ComfyUI nodes and models (CLIP, VAE, UNet, CLIP Vision, StyleModel).\n",
    "    \"\"\"\n",
    "    global clip, vae, unet\n",
    "    global clip_vision, style_model\n",
    "\n",
    "    global CLIPVisionLoader, LoadImage, StyleModelLoader\n",
    "    global CLIPVisionEncode, StyleModelApply\n",
    "    global DualCLIPLoader, VAELoader, CLIPTextEncode\n",
    "    global FluxGuidance, ModelSamplingFlux, BasicGuider\n",
    "    global KSamplerSelect, BasicScheduler, SamplerCustomAdvanced\n",
    "    global RandomNoise, EmptyLatentImage, VAEDecode\n",
    "\n",
    "    # Instantiate node classes\n",
    "    CLIPVisionLoader = NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "    LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "    StyleModelLoader = NODE_CLASS_MAPPINGS[\"StyleModelLoader\"]()\n",
    "    CLIPVisionEncode = NODE_CLASS_MAPPINGS[\"CLIPVisionEncode\"]()\n",
    "    StyleModelApply = NODE_CLASS_MAPPINGS[\"StyleModelApply\"]()\n",
    "\n",
    "    DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "    VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "    CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "    FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "    ModelSamplingFlux = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "    BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "    KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "    BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "    SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "\n",
    "    RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "    EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "    VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "    # Load essential models\n",
    "    with torch.inference_mode():\n",
    "        clip = DualCLIPLoader.load_clip(\n",
    "            \"t5xxl_fp8_e4m3fn.safetensors\",\n",
    "            \"ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors\",\n",
    "            \"flux\"\n",
    "        )[0]\n",
    "        vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "        clip_vision = CLIPVisionLoader.load_clip(\"sigclip_vision_patch14_384.safetensors\")[0]\n",
    "        style_model = StyleModelLoader.load_style_model(\"flux1-redux-dev.safetensors\")[0]\n",
    "\n",
    "    # Load the quantized UNet\n",
    "    UNETLoader = Q_NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
    "    with torch.inference_mode():\n",
    "        unet = UNETLoader.load_unet(\"hyper-flux-16step-Q4_K_M.gguf\")[0]\n",
    "\n",
    "\n",
    "def closestNumber(n, m):\n",
    "    \"\"\"\n",
    "    Finds the closest multiple of m to the number n.\n",
    "    \"\"\"\n",
    "    q = int(n / m)\n",
    "    n1 = m * q\n",
    "    if (n * m) > 0:\n",
    "        n2 = m * (q + 1)\n",
    "    else:\n",
    "        n2 = m * (q - 1)\n",
    "\n",
    "    if abs(n - n1) < abs(n - n2):\n",
    "        return n1\n",
    "    return n2\n",
    "\n",
    "\n",
    "def generate_image(image, prompt, width, height, seed):\n",
    "    \"\"\"\n",
    "    Generates an image by applying a style model based on an input image,\n",
    "    text prompt, and specified dimensions/seed.\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        steps = 16\n",
    "        sampler_name = \"euler\"\n",
    "        scheduler = \"simple\"\n",
    "\n",
    "        # Save the input image to disk so we can load it with ComfyUI's node\n",
    "        image.save(\"/content/ComfyUI/input/image.png\")\n",
    "        loaded_image = LoadImage.load_image(\"image.png\")[0]\n",
    "\n",
    "        clip_vision_output = CLIPVisionEncode.encode(\n",
    "            clip_vision=clip_vision,\n",
    "            image=loaded_image,\n",
    "            crop=\"none\"\n",
    "        )[0]\n",
    "\n",
    "        cond = CLIPTextEncode.encode(text=prompt, clip=clip)[0]\n",
    "        cond_f = FluxGuidance.append(conditioning=cond, guidance=3.5)[0]\n",
    "\n",
    "        cond_ff = StyleModelApply.apply_stylemodel(\n",
    "            conditioning=cond_f,\n",
    "            style_model=style_model,\n",
    "            clip_vision_output=clip_vision_output,\n",
    "            strength=1.0,\n",
    "            strength_type=\"multiply\"\n",
    "        )[0]\n",
    "\n",
    "        noise = RandomNoise.get_noise(seed)[0]\n",
    "        new_unet = ModelSamplingFlux.patch(\n",
    "            model=unet,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            max_shift=1.15,\n",
    "            base_shift=0.5\n",
    "        )[0]\n",
    "\n",
    "        sampler = KSamplerSelect.get_sampler(sampler_name=sampler_name)[0]\n",
    "        guider = BasicGuider.get_guider(model=new_unet, conditioning=cond_ff)[0]\n",
    "        sigmas = BasicScheduler.get_sigmas(\n",
    "            model=new_unet,\n",
    "            scheduler=scheduler,\n",
    "            steps=steps,\n",
    "            denoise=1.0\n",
    "        )[0]\n",
    "\n",
    "        latent_image = EmptyLatentImage.generate(\n",
    "            closestNumber(width, 16),\n",
    "            closestNumber(height, 16)\n",
    "        )[0]\n",
    "\n",
    "        sample, _ = SamplerCustomAdvanced.sample(\n",
    "            noise=noise,\n",
    "            guider=guider,\n",
    "            sampler=sampler,\n",
    "            sigmas=sigmas,\n",
    "            latent_image=latent_image\n",
    "        )\n",
    "        model_management.soft_empty_cache()\n",
    "\n",
    "        decoded = VAEDecode.decode(vae=vae, samples=sample)[0].detach()\n",
    "        output_image = Image.fromarray(\n",
    "            np.array(decoded * 255, dtype=np.uint8)[0]\n",
    "        )\n",
    "        return output_image\n",
    "\n",
    "\n",
    "def on_generate_click(image, prompt, width, height, seed):\n",
    "    \"\"\"\n",
    "    Callback function for the '이미지 생성하기' button in Gradio.\n",
    "    Attempts to generate the image and returns the result and status.\n",
    "    \"\"\"\n",
    "    if not prompt.strip():\n",
    "        return None, \"❌ 텍스트를 입력해주세요.\", None\n",
    "    if not image:\n",
    "        return None, \"❌ 이미지를 입력해주세요.\", None\n",
    "    try:\n",
    "        generated = generate_image(image, prompt, width, height, seed)\n",
    "        return generated, \"✅ 성공적으로 이미지를 생성했습니다.\", seed\n",
    "    except Exception as e:\n",
    "        return None, f\"❌ 에러 발생: {str(e)}\", None\n",
    "\n",
    "\n",
    "def roll_seed():\n",
    "    \"\"\"\n",
    "    Generates a random seed.\n",
    "    \"\"\"\n",
    "    return random.randint(0, 2**32 - 1)\n",
    "\n",
    "\n",
    "def create_interface():\n",
    "    css = \"\"\"\n",
    "    footer {\n",
    "        visibility: hidden;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    interface = gr.Interface(\n",
    "        fn=on_generate_click,\n",
    "        inputs=[\n",
    "            gr.Image(\n",
    "                label=\"🖼️ 이미지\",\n",
    "                type=\"pil\",\n",
    "                height=\"100%\",\n",
    "            ),\n",
    "            gr.Textbox(\n",
    "                label=\"📝 텍스트\",\n",
    "                placeholder=\"원하는 장면을 영어로 입력해주세요...\",\n",
    "                lines=3\n",
    "            ),\n",
    "            gr.Slider(\n",
    "                label=\"📏 가로 (pixels)\",\n",
    "                minimum=64,\n",
    "                maximum=1280,\n",
    "                step=64,\n",
    "                value=1024\n",
    "            ),\n",
    "            gr.Slider(\n",
    "                label=\"📏 세로 (pixels)\",\n",
    "                minimum=64,\n",
    "                maximum=1280,\n",
    "                step=64,\n",
    "                value=1024\n",
    "            ),\n",
    "            gr.Slider(0, 2**32 -1, 2024, label=\"🎲 시드\", step=1)\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Image(label=\"🖼️ 생성한 이미지\"),\n",
    "            gr.Textbox(label=\"🔄 현재 상태\", interactive=False),\n",
    "            gr.Number(label=\"📌 사용된 시드\", value=42, precision=0, interactive=False)\n",
    "        ],\n",
    "        title=\"🖼️ 플럭스 활용하기\",\n",
    "        theme='JohnSmith9982/small_and_pretty',\n",
    "        css=css,\n",
    "        allow_flagging=\"never\",\n",
    "    )\n",
    "\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load all models first\n",
    "    load_models()\n",
    "    # Create and launch Gradio interface\n",
    "    interface = create_interface()\n",
    "    interface.launch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
