{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone code\n",
    "%cd /content\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI.git ./ComfyUI\n",
    "!git clone https://github.com/city96/ComfyUI-GGUF.git ./ComfyUI/custom_nodes/quantized\n",
    "\n",
    "# Download library\n",
    "%cd /content/ComfyUI\n",
    "!pip install -q einops==0.8.0 torchsde==0.2.6 diffusers==0.31.0 accelerate==1.1.0 gguf==0.10.0 gradio==5.9.1\n",
    "!pip install -q huggingface_hub==0.27.0 hf_transfer==0.1.8\n",
    "\n",
    "# Download model\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download martintomov/Hyper-FLUX.1-dev-gguf hyper-flux-16step-Q4_K_M.gguf --local-dir ./models/unet --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Comfy-Org/sigclip_vision_384 sigclip_vision_patch14_384.safetensors --local-dir ./models/clip_vision --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Thelocallab/Flux-Dev-Redux flux1-redux-dev.safetensors --local-dir ./models/style_models --local-dir-use-symlinks False # originally, it's from black-forest-labs/FLUX.1-Redux-dev. Check the LICENSE.\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download camenduru/FLUX.1-dev ae.sft --local-dir ./models/vae --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download zer0int/CLIP-GmP-ViT-L-14 ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors --local-dir ./models/clip --local-dir-use-symlinks False\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download comfyanonymous/flux_text_encoders t5xxl_fp8_e4m3fn.safetensors --local-dir ./models/clip --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nodes\n",
    "%cd /content/ComfyUI\n",
    "\n",
    "import torch\n",
    "import nodes\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from comfy import model_management\n",
    "from comfy_extras import nodes_custom_sampler, nodes_flux, nodes_model_advanced\n",
    "\n",
    "CLIPVisionLoader = nodes.NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "LoadImage = nodes.NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "StyleModelLoader = nodes.NODE_CLASS_MAPPINGS[\"StyleModelLoader\"]()\n",
    "CLIPVisionEncode = nodes.NODE_CLASS_MAPPINGS[\"CLIPVisionEncode\"]()\n",
    "StyleModelApply = nodes.NODE_CLASS_MAPPINGS[\"StyleModelApply\"]()\n",
    "\n",
    "DualCLIPLoader = nodes.NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
    "VAELoader = nodes.NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "CLIPTextEncode = nodes.NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "ModelSamplingFlux = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "\n",
    "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "EmptyLatentImage = nodes.NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "VAEDecode = nodes.NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "# Import model\n",
    "with torch.inference_mode():\n",
    "  clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"ViT-L-14-TEXT-detail-improved-hiT-GmP-TE-only-HF.safetensors\", \"flux\")[0]\n",
    "  vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "  clip_vision = CLIPVisionLoader.load_clip(\"sigclip_vision_patch14_384.safetensors\")[0]\n",
    "  style_model = StyleModelLoader.load_style_model(\"flux1-redux-dev.safetensors\")[0]\n",
    "\n",
    "from custom_nodes.quantized.nodes import NODE_CLASS_MAPPINGS\n",
    "\n",
    "UNETLoader = NODE_CLASS_MAPPINGS[\"UnetLoaderGGUF\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  unet = UNETLoader.load_unet(\"hyper-flux-16step-Q4_K_M.gguf\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "def closestNumber(n, m):\n",
    "    q = int(n / m)\n",
    "    n1 = m * q\n",
    "    if (n * m) > 0:\n",
    "        n2 = m * (q + 1)\n",
    "    else:\n",
    "        n2 = m * (q - 1)\n",
    "    if abs(n - n1) < abs(n - n2):\n",
    "        return n1\n",
    "    return n2\n",
    "\n",
    "# Function to generate image\n",
    "def generate_image(image, prompt, width, height, seed):\n",
    "  with torch.inference_mode():\n",
    "    positive_prompt = prompt\n",
    "    width = width\n",
    "    height = height\n",
    "    seed = seed\n",
    "    steps = 16\n",
    "    sampler_name = \"euler\"\n",
    "    scheduler = \"simple\"\n",
    "\n",
    "    image.save(\"/content/ComfyUI/input/image.png\")\n",
    "    image = LoadImage.load_image(\"image.png\")[0]\n",
    "    clip_vision_output = CLIPVisionEncode.encode(clip_vision=clip_vision, image=image, crop=\"none\")[0]\n",
    "\n",
    "    cond = CLIPTextEncode.encode(text=prompt, clip=clip)[0]\n",
    "    cond_f = FluxGuidance.append(conditioning=cond, guidance=3.5)[0]\n",
    "    cond_ff = StyleModelApply.apply_stylemodel(conditioning=cond_f, style_model=style_model, clip_vision_output=clip_vision_output, strength=1.0, strength_type=\"multiply\")[0]\n",
    "\n",
    "    noise = RandomNoise.get_noise(seed)[0]\n",
    "    new_unet = ModelSamplingFlux.patch(model=unet, width=width, height=height, max_shift=1.15, base_shift=0.5)[0]\n",
    "\n",
    "    sampler = KSamplerSelect.get_sampler(sampler_name=sampler_name)[0]\n",
    "    guider = BasicGuider.get_guider(model=new_unet, conditioning=cond_ff)[0]\n",
    "    sigmas = BasicScheduler.get_sigmas(model=new_unet, scheduler=scheduler, steps=steps, denoise=1.0)[0]\n",
    "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
    "\n",
    "    sample, _ = SamplerCustomAdvanced.sample(noise=noise, guider=guider, sampler=sampler, sigmas=sigmas, latent_image=latent_image)\n",
    "    model_management.soft_empty_cache()\n",
    "\n",
    "    decoded = VAEDecode.decode(vae=vae, samples=sample)[0].detach()\n",
    "    image = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])\n",
    "    return image\n",
    "\n",
    "# Function to handle button click\n",
    "def on_generate_click(image, prompt, width, height, seed):\n",
    "    if not prompt.strip():\n",
    "        return None, \"❌ 텍스트를 입력해주세요.\", None\n",
    "    if not image:\n",
    "        return None, \"❌ 이미지를 입력해주세요.\", None\n",
    "    try:\n",
    "        image = generate_image(image, prompt, width, height, seed)\n",
    "        return image, \"✅ 성공적으로 이미지를 생성했습니다.\", seed\n",
    "    except Exception as e:\n",
    "        return None, f\"❌ 에러 발생: {str(e)}\", None\n",
    "\n",
    "# Function to generate a random seed\n",
    "def roll_seed():\n",
    "    return random.randint(0, 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "def create_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# 🖼️ 플럭스 활용하기\")\n",
    "        with gr.Row():\n",
    "            # Left column: Inputs\n",
    "            with gr.Column(scale=1):\n",
    "                image = gr.Image(\n",
    "                    label=\"🖼️ 이미지\",\n",
    "                    type=\"pil\",\n",
    "                    height=300,\n",
    "                )\n",
    "                prompt = gr.Textbox(\n",
    "                    label=\"📝 텍스트\",\n",
    "                    placeholder=\"원하는 장면을 영어로 입력해주세요...\",\n",
    "                    lines=3\n",
    "                )\n",
    "                width = gr.Slider(\n",
    "                    label=\"📏 가로 (pixels)\",\n",
    "                    minimum=64,\n",
    "                    maximum=1280,\n",
    "                    step=64,\n",
    "                    value=1024\n",
    "                )\n",
    "                height = gr.Slider(\n",
    "                    label=\"📏 세로 (pixels)\",\n",
    "                    minimum=64,\n",
    "                    maximum=1280,\n",
    "                    step=64,\n",
    "                    value=1024\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    seed = gr.Number(\n",
    "                        label=\"🎲 시드\",\n",
    "                        value=2024,\n",
    "                        precision=0,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    roll_seed_btn = gr.Button(\"🎲 랜덤 시드 만들기\")\n",
    "                generate_btn = gr.Button(\"🎨 이미지 생성하기\")\n",
    "                progress = gr.Textbox(label=\"🔄 현재 상태\", interactive=False)\n",
    "\n",
    "            # Right column: Output\n",
    "            with gr.Column(scale=1):\n",
    "                output = gr.Image(label=\"🖼️ 생성한 이미지\")\n",
    "                used_seed = gr.Number(label=\"📌 사용된 시드\", value=42, precision=0, interactive=False)\n",
    "\n",
    "        # Define the button click events\n",
    "        generate_btn.click(\n",
    "            fn=on_generate_click,\n",
    "            inputs=[image, prompt, width, height, seed],\n",
    "            outputs=[output, progress, used_seed]\n",
    "        )\n",
    "\n",
    "        roll_seed_btn.click(\n",
    "            fn=roll_seed,\n",
    "            inputs=None,\n",
    "            outputs=seed\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface = create_interface()\n",
    "    interface.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
